Group 09 - Final Project
Isaac Woods
Ryan Racine
Aiden O'Keefe

Describe how you solved each variant
Describe the structure of your code: did you reuse stuff across variants?
No need for an intro in which you describe the goal of the project.

Code Structure
------------------------------------------
Our final approach for this project was to use approximate Q-learning to solve each variant with a set of 12 features. The main function, 'do', decides which action to take by calling a function named 'calcQ(world, action)'. calcQ can be passed an action or a null action (represented by the value -1). If a null action is passed, it evaluates the features in the current state, then evaluates state changes (which will be explained later).  If calcQ receives a valid action as a parameter, it simulates the world with the given move, then calculates the new world's Q value. If the result is a terminal state, the reward from this state is returned.

When our agent is created, you can enable or disable features from the total Q-learning function to tailor the agent to the requirements of each variant. Any weights which are set to 0 when instantiating the agent represent the features that will be disabled during the default state. A 2nd state exists for the agent called the "exit rush" state, which the agent will enter when it has determined via A* that it is closer to the exit than any enemy present on the map is. In this case, all other features are disabled and the agent simply moves to the exit.

calcQ operates by calling a general function (calcFeatureN()) to calculate a given feature value. This function calls a set of individual calcFeature functions specific to each feature that will return the feature value. Below is a list of the features used and their function:

Features
------------------------------------------
feature0: This feature is a dummy feature which represents the bias.
feature1: This feature is based on the Manhattan distance to the exit.
feature2: This feature is based on the Manhattan distance to a bomb in the current column or row. This allows us to avoid bombs by ensure we are at least a minimum distance away if we have to remain in the same column or row as a bomb.
feature3: This feature is the number of neighboring walls. This allows us to find locations that allow us to destroy the most walls with the least bombs.
feature4: This feature is the distance from the closest side wall. This helps us ensure we aren't trapping ourselves.
feature5: This feature is the number of bombs on the field. This lets us know if dropping a bomb is a valid option or not.
feature6: This feature is our A* pathfinding algorithm.
feature7: This feature is based on the closest enemy to the player at a given time.
feature8: This feature is based on the closest enemy to the player within a range of 3 tiles. This lets the agent know of any eminent danger.
feature9: This feature detects corners.
feature10: This feature looks for enemies within a Manhattan distance of 6.
feature11: This feature detects enemies in immediate range of us.

After each feature has been evaluated, they are multiplied by the weights we have provided, giving us the final Q score for a given Q(s, a). We choose the action with the highest score and take it.

Solution
------------------------------------------
Our Q-learning agent was reused for each variant with slight changes to which features were activated per variant. For instance, Variant 1 does not require any of the enemy features to be enabled, so they are left off to save on processing time. We added 3 things to the character's constructor: "active_features", which sets the initial weights and determines which features have been enabled; decay, which is used during the training process; and lr, which is also used during the training process.

We first attempted to allow the agent to learn on its own. However, we found that the learning process was too slow to feasibly finish this project in time. As a result, we used a "human-in-the-loop" training process where the agent's progress would be observed, and weights would be adjusted by one of the group members to help speed up the learning process. This process took approximately 30 minutes to an hour per variant, with about 15-20 minutes added on to tweak which features would be enabled and disabled during A/B testing. The final weights for each variant included in our submission result in these results over 100 games:

Results
------------------------------
Key:
Variant #: Wins:Games (win%) 

Scenario 1:
Variant 1: 100:100 (100%)
Variant 2: 99:100 (99%)
Variant 3: 93:99 (93.94%)
Variant 4: 80:100 (80%)
Variant 5: 85:100 (85%)

Scenario 2:
Variant 1: 100:100 (100%)
Variant 2: 100:100 (100%)
Variant 3: 100:100 (100%)
Variant 4: 96:100 (96%)
Variant 5: 88:100 (88%)

Conclusions
------------------------------------------
Our agent performed very well across both scenarios in all variants, with a slight drop-off in performance in the more complex variants (Variant 4 and 5). We have surpassed or matched the 80% threshold in each and every variant however and are satisfied with our performance. Approximate Q-learning based on toggleable features is a lightweight, easy to implement solution for a complex problem like Bomberman that has yielded fantastic results for us.

One interesting experiment would be to see, after extensive training, if the approximate Q-learning we implemented would converge on the same values we ended up with after our human-in-the-loop learning process. We believe that, while our weights are very good and win a significant portion of the time, there likely exist more optimal weights that could approach a 100% win percentage over the same 100 games per each scenario. The main limiting factor, and why we did not complete this during the project, was a lack of time to train the agent. 